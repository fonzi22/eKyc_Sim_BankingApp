{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-29T21:54:05.311830Z",
     "iopub.status.busy": "2025-11-29T21:54:05.310930Z",
     "iopub.status.idle": "2025-11-29T21:54:16.687445Z",
     "shell.execute_reply": "2025-11-29T21:54:16.686465Z",
     "shell.execute_reply.started": "2025-11-29T21:54:05.311796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  \n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. CẤU HÌNH\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    'root_dir': '/kaggle/input/finaldifaattack',\n",
    "    'img_size': 224,\n",
    "    'seq_len': 8,           # Số frame lấy từ video\n",
    "    'batch_size': 4,        # Giảm batch size nếu video nặng\n",
    "    'num_epochs': 20,\n",
    "    'lr': 0.001,\n",
    "    'imposter_rate': 0.3,   # Tỷ lệ tạo cặp sai người\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. VIDEO LOADER\n",
    "# ==========================================\n",
    "def load_frames_from_video(video_path, seq_len=8):\n",
    "    \"\"\"\n",
    "    Đọc video file và lấy ngẫu nhiên/đều seq_len frames.\n",
    "    Trả về list các PIL Image.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Nếu video quá ngắn, lấy tất cả rồi lặp lại\n",
    "    if total_frames < 1:\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    # Chọn indices để lấy frame\n",
    "    indices = np.linspace(0, total_frames - 1, seq_len).astype(int)\n",
    "    \n",
    "    current_idx = 0\n",
    "    collected_count = 0\n",
    "    \n",
    "    # Duyệt video để lấy đúng frame tại indices\n",
    "    # (Cách này tối ưu hơn đọc hết vào RAM)\n",
    "    indices_set = set(indices)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if current_idx in indices_set:\n",
    "            # Convert BGR (OpenCV) -> RGB (PIL)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            collected_count += 1\n",
    "        \n",
    "        current_idx += 1\n",
    "        if collected_count >= seq_len:\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    \n",
    "    # Padding nếu thiếu frame (trường hợp video lỗi header)\n",
    "    while len(frames) < seq_len:\n",
    "        if len(frames) > 0:\n",
    "            frames.append(frames[-1])\n",
    "        else:\n",
    "            # Video hỏng hoàn toàn -> tạo ảnh đen\n",
    "            frames.append(Image.new('RGB', (CONFIG['img_size'], CONFIG['img_size'])))\n",
    "            \n",
    "    return frames\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATASET \n",
    "# ==========================================\n",
    "class eKYCDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, seq_len=8, imposter_rate=0.3):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.imposter_rate = imposter_rate\n",
    "        \n",
    "        self.samples = []       # Danh sách các mẫu video để train\n",
    "        self.person_id_map = {} # Dict lưu list ID của mỗi người: {'Person_001': ['id1.jpg', 'id2.jpg']}\n",
    "        \n",
    "        self._prepare_db()\n",
    "\n",
    "    def _prepare_db(self):\n",
    "        # 1. Quét toàn bộ thư mục ID trước để xây dựng database ID\n",
    "        id_root = os.path.join(self.root_dir, 'ID')\n",
    "        if os.path.exists(id_root):\n",
    "            for person in os.listdir(id_root):\n",
    "                p_path = os.path.join(id_root, person)\n",
    "                if not os.path.isdir(p_path): continue\n",
    "                \n",
    "                # Lấy tất cả ảnh trong folder ID của người đó\n",
    "                images = []\n",
    "                for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "                    images.extend(glob.glob(os.path.join(p_path, ext)))\n",
    "                \n",
    "                if len(images) > 0:\n",
    "                    self.person_id_map[person] = images\n",
    "\n",
    "        # 2. Load Real Videos\n",
    "        real_root = os.path.join(self.root_dir, 'Real')\n",
    "        if os.path.exists(real_root):\n",
    "            for person in os.listdir(real_root):\n",
    "                if person not in self.person_id_map: continue # Bỏ qua nếu không có ID\n",
    "                \n",
    "                p_path = os.path.join(real_root, person)\n",
    "                # Lấy tất cả video\n",
    "                videos = []\n",
    "                for ext in ['*.mp4', '*.avi', '*.mov']:\n",
    "                    videos.extend(glob.glob(os.path.join(p_path, ext)))\n",
    "                \n",
    "                for vid in videos:\n",
    "                    self.samples.append({\n",
    "                        'type': 'real',\n",
    "                        'video_path': vid,\n",
    "                        'person_name': person,\n",
    "                        # Map sang file depth tương ứng (giả sử depth cũng là video cùng tên)\n",
    "                        'depth_path': os.path.join(self.root_dir, 'Real_Depth', person, os.path.basename(vid))\n",
    "                    })\n",
    "\n",
    "        # 3. Load Fake Videos\n",
    "        fake_root = os.path.join(self.root_dir, 'Fake')\n",
    "        attack_types = ['Print-attack', 'Video-replay']\n",
    "        for att in attack_types:\n",
    "            att_path = os.path.join(fake_root, att)\n",
    "            if not os.path.exists(att_path): continue\n",
    "            \n",
    "            for person in os.listdir(att_path):\n",
    "                # Fake phải attack vào người có ID\n",
    "                if person not in self.person_id_map: continue \n",
    "                \n",
    "                p_path = os.path.join(att_path, person)\n",
    "                videos = []\n",
    "                for ext in ['*.mp4', '*.avi', '*.mov']:\n",
    "                    videos.extend(glob.glob(os.path.join(p_path, ext)))\n",
    "                    \n",
    "                for vid in videos:\n",
    "                    self.samples.append({\n",
    "                        'type': 'fake',\n",
    "                        'video_path': vid,\n",
    "                        'person_name': person,\n",
    "                        'depth_path': None\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        person_name = sample['person_name']\n",
    "        \n",
    "        # --- A. Lấy ID Image ---\n",
    "        # Logic Imposter: Real thì có tỉ lệ bị tráo ID, Fake thì dùng đúng ID của victim\n",
    "        is_imposter = False\n",
    "        target_name = person_name\n",
    "        \n",
    "        if sample['type'] == 'real':\n",
    "            # Nếu random trúng imposter, chọn ID của người khác\n",
    "            if random.random() < self.imposter_rate and len(self.person_id_map) > 1:\n",
    "                is_imposter = True\n",
    "                possible_imposters = list(self.person_id_map.keys())\n",
    "                possible_imposters.remove(person_name)\n",
    "                target_name = random.choice(possible_imposters)\n",
    "        \n",
    "        # Chọn ngẫu nhiên 1 trong các ảnh ID của người được chọn (Target)\n",
    "        # Việc random này giúp model không overfit vào 1 ảnh ID duy nhất\n",
    "        id_path = random.choice(self.person_id_map[target_name])\n",
    "        \n",
    "        try:\n",
    "            id_img = Image.open(id_path).convert('RGB')\n",
    "            \n",
    "            # --- B. Load Video Frames ---\n",
    "            video_frames = load_frames_from_video(sample['video_path'], self.seq_len)\n",
    "            if video_frames is None: \n",
    "                raise Exception(\"Video load error\")\n",
    "\n",
    "            # --- C. Load Depth ---\n",
    "            w, h = video_frames[0].size\n",
    "            if sample['type'] == 'real':\n",
    "                # Load Depth từ video depth hoặc fallback tạo đen\n",
    "                if os.path.exists(sample['depth_path']):\n",
    "                    depth_frames = load_frames_from_video(sample['depth_path'], self.seq_len)\n",
    "                    if depth_frames:\n",
    "                        depth_frames = [d.convert('L') for d in depth_frames]\n",
    "                    else:\n",
    "                        depth_frames = [Image.new('L', (w, h), 0) for _ in range(self.seq_len)]\n",
    "                else:\n",
    "                    # Nếu không tìm thấy file video depth\n",
    "                    depth_frames = [Image.new('L', (w, h), 0) for _ in range(self.seq_len)]\n",
    "            else:\n",
    "                # Fake -> Depth = 0\n",
    "                depth_frames = [Image.new('L', (w, h), 0) for _ in range(self.seq_len)]\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading {sample['video_path']}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.samples))\n",
    "\n",
    "        # --- D. Labels ---\n",
    "        label_live = 1.0 if sample['type'] == 'real' else 0.0\n",
    "        label_match = 0.0 if (sample['type'] == 'real' and is_imposter) else 1.0\n",
    "\n",
    "        # --- E. Transform ---\n",
    "        depth_trans = transforms.Compose([\n",
    "            transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        if self.transform:\n",
    "            id_tensor = self.transform(id_img)\n",
    "            video_tensor = torch.stack([self.transform(img) for img in video_frames])\n",
    "        else:\n",
    "            t = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "            id_tensor = t(id_img)\n",
    "            video_tensor = torch.stack([t(img) for img in video_frames])\n",
    "            \n",
    "        depth_tensor = torch.stack([depth_trans(d) for d in depth_frames])\n",
    "\n",
    "        return {\n",
    "            'id': id_tensor,\n",
    "            'video': video_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'label_live': torch.tensor([label_live], dtype=torch.float32),\n",
    "            'label_match': torch.tensor([label_match], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODEL \n",
    "# ==========================================\n",
    "class MobileKYCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "        self.encoder = list(backbone.children())[0] \n",
    "        self.feature_dim = 576 \n",
    "        \n",
    "        self.depth_conv = nn.Conv2d(self.feature_dim, 128, kernel_size=1)\n",
    "        self.depth_estimator = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        self.reduce_dim = nn.Linear(self.feature_dim, 128)\n",
    "        self.rnn = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.liveness_head = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5), nn.Linear(64, 1)\n",
    "        )\n",
    "        self.matching_head = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5), nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        feat_map = self.encoder(x)\n",
    "        feat_vec = F.adaptive_avg_pool2d(feat_map, (1, 1)).flatten(1)\n",
    "        return feat_map, feat_vec\n",
    "\n",
    "    def forward(self, id_img, video_frames):\n",
    "        batch_size, seq_len, c, h, w = video_frames.size()\n",
    "        \n",
    "        # ID Feat\n",
    "        _, id_vec = self.forward_backbone(id_img)\n",
    "        id_vec_reduced = self.reduce_dim(id_vec)\n",
    "        \n",
    "        # Video Feat\n",
    "        video_reshaped = video_frames.view(batch_size * seq_len, c, h, w)\n",
    "        vid_feat_map, vid_feat_vec = self.forward_backbone(video_reshaped)\n",
    "        \n",
    "        # Depth & Attention\n",
    "        depth_input = self.depth_conv(vid_feat_map)\n",
    "        pred_depth_small = self.depth_estimator(depth_input)\n",
    "        depth_score = pred_depth_small.mean(dim=[2, 3]).view(batch_size * seq_len, 1)\n",
    "        \n",
    "        vid_feat_vec_attended = vid_feat_vec * (0.5 + depth_score)\n",
    "        vid_vec_reduced = self.reduce_dim(vid_feat_vec_attended)\n",
    "        \n",
    "        # Differencing\n",
    "        vid_vec_seq = vid_vec_reduced.view(batch_size, seq_len, -1)\n",
    "        id_vec_expanded = id_vec_reduced.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        diff_feat = torch.abs(vid_vec_seq - id_vec_expanded)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_input = torch.cat([vid_vec_seq, diff_feat], dim=2)\n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "        final_feat = rnn_out[:, -1, :]\n",
    "        \n",
    "        liveness_logit = self.liveness_head(final_feat)\n",
    "        matching_logit = self.matching_head(final_feat)\n",
    "        \n",
    "        # Depth Loss Output\n",
    "        pred_depth_full = F.interpolate(pred_depth_small, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        pred_depth_full = pred_depth_full.view(batch_size, seq_len, 1, h, w)\n",
    "        \n",
    "        return liveness_logit, matching_logit, pred_depth_full\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAINING LOOP \n",
    "# ==========================================\n",
    "def train_model():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"Scanning dataset (Video Files)... This may take a moment.\")\n",
    "    dataset = eKYCDataset(root_dir=CONFIG['root_dir'], transform=transform, seq_len=CONFIG['seq_len'])\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"No samples found! Check directory structure.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(dataset)} video samples.\")\n",
    "        print(f\"Found IDs for {len(dataset.person_id_map)} people.\")\n",
    "\n",
    "    # Num_workers nên set thấp nếu đọc video vì tốn CPU/IO\n",
    "    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    \n",
    "    model = MobileKYCModel().to(CONFIG['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    criterion_cls = nn.BCEWithLogitsLoss()\n",
    "    criterion_depth = nn.MSELoss()\n",
    "    \n",
    "    print(\"Start Training...\")\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        acc_live = 0\n",
    "        acc_match = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            id_img = batch['id'].to(CONFIG['device'])\n",
    "            video = batch['video'].to(CONFIG['device'])\n",
    "            gt_depth = batch['depth'].to(CONFIG['device'])\n",
    "            lbl_live = batch['label_live'].to(CONFIG['device'])\n",
    "            lbl_match = batch['label_match'].to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out_live, out_match, pred_depth = model(id_img, video)\n",
    "            \n",
    "            # Loss Calculation\n",
    "            loss_live = criterion_cls(out_live, lbl_live)\n",
    "            loss_match = criterion_cls(out_match, lbl_match)\n",
    "            # Chỉ tính depth loss cho mẫu Real để tránh nhiễu, hoặc tính cả Fake (depth=0) cũng được\n",
    "            loss_depth = criterion_depth(pred_depth, gt_depth)\n",
    "            \n",
    "            total_loss = loss_live + loss_match + 0.5 * loss_depth\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            acc_live += ((torch.sigmoid(out_live) > 0.5).float() == lbl_live).sum().item()\n",
    "            acc_match += ((torch.sigmoid(out_match) > 0.5).float() == lbl_match).sum().item()\n",
    "            total_samples += lbl_live.size(0)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Ep {epoch+1} [{i}/{len(dataloader)}] Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        print(f\"=== Epoch {epoch+1} ===\")\n",
    "        print(f\"Loss: {running_loss/len(dataloader):.4f}\")\n",
    "        print(f\"Liveness Acc: {acc_live/total_samples:.4f}\")\n",
    "        print(f\"Matching Acc: {acc_match/total_samples:.4f}\")\n",
    "        \n",
    "    torch.save(model.state_dict(), \"ekyc_mobile_video_model.pth\")\n",
    "    print(\"Model Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T21:54:44.476960Z",
     "iopub.status.busy": "2025-11-29T21:54:44.476590Z",
     "iopub.status.idle": "2025-11-29T21:54:48.393244Z",
     "shell.execute_reply": "2025-11-29T21:54:48.392379Z",
     "shell.execute_reply.started": "2025-11-29T21:54:44.476915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert thành công! File 'ekyc_model_mobile.ptl' đã sẵn sàng.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils import mobile_optimizer\n",
    "\n",
    "# 1. Load Model đã train\n",
    "device = 'cpu'\n",
    "model = MobileKYCModel()\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/ekyc-training/ekyc_mobile_video_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 2. Tạo Wrapper để loại bỏ Depth Output và thêm Sigmoid\n",
    "class InferenceWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        self.model = original_model\n",
    "        \n",
    "    def forward(self, id_img, video_frames):\n",
    "        # Model gốc trả về: liveness_logit, matching_logit, pred_depth\n",
    "        live_logit, match_logit, _ = self.model(id_img, video_frames)\n",
    "        \n",
    "        # Chuyển Logit -> Probability (0.0 đến 1.0)\n",
    "        live_score = torch.sigmoid(live_logit)\n",
    "        match_score = torch.sigmoid(match_logit)\n",
    "        return live_score, match_score\n",
    "\n",
    "wrapper_model = InferenceWrapper(model)\n",
    "wrapper_model.eval()\n",
    "\n",
    "# 3. Tạo Dummy Input (Dữ liệu giả để trace mô hình)\n",
    "# ID: [1, 3, 224, 224]\n",
    "dummy_id = torch.randn(1, 3, 224, 224)\n",
    "# Video: [1, 8, 3, 224, 224] (Batch=1, Seq=8)\n",
    "dummy_video = torch.randn(1, 8, 3, 224, 224)\n",
    "\n",
    "# 4. Trace và Save (Quan trọng: dùng torch.jit.trace)\n",
    "traced_script_module = torch.jit.trace(wrapper_model, (dummy_id, dummy_video))\n",
    "\n",
    "# Tối ưu hóa cho Mobile\n",
    "traced_script_module_optimized = torch.utils.mobile_optimizer.optimize_for_mobile(traced_script_module)\n",
    "\n",
    "# Lưu file .ptl (PyTorch Lite)\n",
    "traced_script_module_optimized._save_for_lite_interpreter(\"ekyc_model_mobile.ptl\")\n",
    "\n",
    "print(\"Convert thành công! File 'ekyc_model_mobile.ptl' đã sẵn sàng.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 282726674,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
